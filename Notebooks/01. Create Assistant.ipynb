{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-02T22:05:39.077891Z",
     "start_time": "2025-09-02T22:05:37.389217Z"
    }
   },
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from markdown import Markdown\n",
    "import pandas as pd\n",
    "import re\n",
    "import tqdm\n",
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from agents import Agent, FileSearchTool, Runner, trace\n",
    "from IPython.display import display, Markdown\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Pre-Process Excel Files",
   "id": "e316476b2f4473"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:05:39.085807Z",
     "start_time": "2025-09-02T22:05:39.081896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ALLOWED_EXTENSIONS = {\n",
    "    \".c\", \".cpp\", \".cs\", \".css\", \".doc\", \".docx\", \".go\", \".html\", \".java\",\n",
    "    \".js\", \".json\", \".md\", \".pdf\", \".php\", \".pptx\", \".py\", \".rb\", \".sh\",\n",
    "    \".tex\", \".ts\", \".txt\"\n",
    "}\n",
    "\n",
    "def get_all_file_paths(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively get all file paths in a folder and its subfolders.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str or Path): Path to the root folder.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full file paths.\n",
    "    \"\"\"\n",
    "    return [str(file) for file in Path(folder_path).rglob('*') if file.is_file()]\n",
    "\n",
    "def get_allowed_file_paths(folder_path):\n",
    "    \"\"\"\n",
    "    Recursively get file paths in a folder and its subfolders,\n",
    "    filtered by allowed extensions.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str or Path): Path to the root folder.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full file paths matching the allowed extensions.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        str(file)\n",
    "        for file in Path(folder_path).rglob('*')\n",
    "        if file.is_file() and file.suffix.lower() in ALLOWED_EXTENSIONS\n",
    "    ]\n",
    "\n",
    "folder = \"../Data/Knowledge_Base/Raw\"\n",
    "all_file_paths = get_all_file_paths(folder)\n",
    "allowed_file_paths = get_allowed_file_paths(folder)\n",
    "print(\"Number of files:\", len(all_file_paths))\n",
    "print(\"Number of Allowed files:\", len(allowed_file_paths))"
   ],
   "id": "5a3124c2c7cfeadb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 7\n",
      "Number of Allowed files: 1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Convert XLSX files to json",
   "id": "a97438c41fc23e41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:05:39.192681Z",
     "start_time": "2025-09-02T22:05:39.189032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def excel_to_json_all_sheets(\n",
    "    excel_path: str | Path,\n",
    "    intermediate_dir: str | Path = \"..//Data/Knowledge_Base/Intermediate\",\n",
    "    orient: str = \"records\",\n",
    "    date_format: str = \"iso\",\n",
    "    keep_index: bool = False\n",
    ") -> list[Path]:\n",
    "    \"\"\"\n",
    "    Convert every sheet in an Excel file to a JSON file in an intermediate folder.\n",
    "\n",
    "    - Each sheet -> one JSON file.\n",
    "    - JSON is an array of rows (orient='records' by default).\n",
    "    - NaN -> null in JSON.\n",
    "    - Dates -> ISO-8601 strings by default.\n",
    "\n",
    "    Args:\n",
    "        excel_path: Path to the .xlsx/.xls/.xlsm file.\n",
    "        intermediate_dir: Folder to save JSON files (created if missing).\n",
    "        orient: pandas DataFrame.to_json orient (default 'records').\n",
    "        date_format: 'iso' (default) or 'epoch' for timestamps.\n",
    "        keep_index: Whether to include the DataFrame index in the JSON.\n",
    "\n",
    "    Returns:\n",
    "        List of Paths to the saved JSON files.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the Excel file does not exist.\n",
    "        ValueError: If no sheets are found.\n",
    "    \"\"\"\n",
    "    excel_path = Path(excel_path)\n",
    "    if not excel_path.exists():\n",
    "        raise FileNotFoundError(f\"Excel file not found: {excel_path}\")\n",
    "\n",
    "    out_dir = Path(intermediate_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Read all sheets into a dict of {sheet_name: DataFrame}\n",
    "    # sheet_name=None -> read all; engine auto-detected (requires openpyxl for .xlsx)\n",
    "    sheets = pd.read_excel(excel_path, sheet_name=None)\n",
    "\n",
    "    if not sheets:\n",
    "        raise ValueError(f\"No sheets found in {excel_path}\")\n",
    "\n",
    "    def _sanitize(name: str) -> str:\n",
    "        # Keep it filesystem-friendly and stable\n",
    "        name = name.strip()\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)          # spaces -> underscores\n",
    "        name = re.sub(r\"[^A-Za-z0-9._-]\", \"\", name)  # drop unsafe chars\n",
    "        return name or \"Sheet\"\n",
    "\n",
    "    saved_paths: list[Path] = []\n",
    "    base = excel_path.stem\n",
    "\n",
    "    for sheet_name, df in sheets.items():\n",
    "        # Ensure consistent nulls in JSON; optional: drop all-empty columns/rows\n",
    "        # df = df.dropna(how=\"all\").loc[:, df.dropna(how=\"all\").columns]\n",
    "        safe_sheet = _sanitize(str(sheet_name))\n",
    "        out_path = out_dir / f\"{base}__{safe_sheet}.json\"\n",
    "\n",
    "        # Write JSON\n",
    "        df.to_json(\n",
    "            out_path,\n",
    "            orient=orient,\n",
    "            date_format=date_format,\n",
    "            force_ascii=False,  # keep Unicode characters\n",
    "            index=keep_index\n",
    "        )\n",
    "        saved_paths.append(out_path)\n",
    "\n",
    "    return saved_paths"
   ],
   "id": "8e0117ae9c09f153",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:05:43.295187Z",
     "start_time": "2025-09-02T22:05:39.197512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for file in tqdm.tqdm(all_file_paths):\n",
    "    if file.lower().endswith((\".xlsx\", \".xlrd\", \".xls\", \".xlsm\")):\n",
    "        excel_to_json_all_sheets(file)"
   ],
   "id": "920ab545894ee5aa",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]/Users/Francisco_Reveriano/Documents/Truist/Workforce_Model/.venv/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      " 71%|███████▏  | 5/7 [00:03<00:01,  1.27it/s]/Users/Francisco_Reveriano/Documents/Truist/Workforce_Model/.venv/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "/Users/Francisco_Reveriano/Documents/Truist/Workforce_Model/.venv/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
      "100%|██████████| 7/7 [00:04<00:00,  1.71it/s]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Assisstant",
   "id": "ca0ee82359416940"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:06:49.284847Z",
     "start_time": "2025-09-02T22:05:43.308534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_API_BASE\"))\n",
    "# 1) Create a vector store to hold your PDFs\n",
    "vs = client.vector_stores.create(name=\"Project PDFs\")\n",
    "# 2) Batch-upload many PDFs and wait until indexing is done\n",
    "directory = \"../Data/Knowledge_Base\"\n",
    "file_paths = get_allowed_file_paths(directory)\n",
    "print(f\"Uploading {len(file_paths)} Files...\")\n",
    "file_streams = [open(p, \"rb\") for p in file_paths]\n",
    "try:\n",
    "    batch = client.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vs.id,\n",
    "        files=file_streams,  # any number of local PDFs\n",
    "    )\n",
    "    print(batch.status, batch.file_counts)  # e.g., \"completed\" and counts\n",
    "finally:\n",
    "    for f in file_streams:\n",
    "        f.close()"
   ],
   "id": "73e4cf23977819db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../Data/Knowledge_Base/Intermediate/Full_Hash_File (1)__FULL_HASH_FILE.json', '../Data/Knowledge_Base/Intermediate/Masked Extended_Worker IAM SDLC 06-30-25 (1)__IAM_L_Lang.json', '../Data/Knowledge_Base/Intermediate/Masked Extended_Worker IAM SDLC 06-30-25 (1)__SDLC_B_Stalnaker.json', '../Data/Knowledge_Base/Intermediate/ZBB_Data_Package (1)__Project_Info.json', '../Data/Knowledge_Base/Intermediate/AI_FILE1 (1)__AI_FILE.json', '../Data/Knowledge_Base/Intermediate/ZBB File (1)__ZBB_FILE.json', '../Data/Knowledge_Base/Intermediate/ZBB_Data_Package (1)__All_Team_Activities.json', '../Data/Knowledge_Base/Intermediate/ZBB_Data_Package (1)__Worker_Alignment_to_Team_.json', '../Data/Knowledge_Base/Intermediate/ZBB_Data_Package (1)__Team_Information.json', '../Data/Knowledge_Base/Intermediate/AI_FILE (1)__AI_FILE.json', '../Data/Knowledge_Base/Raw/June 2025 ST Monthly TDLC Metrics Report.pdf']\n",
      "Uploading 11 Files...\n",
      "completed FileCounts(cancelled=0, completed=10, failed=1, in_progress=0, total=11)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:06:49.303063Z",
     "start_time": "2025-09-02T22:06:49.301138Z"
    }
   },
   "cell_type": "code",
   "source": "vs.id",
   "id": "91a8d5a9b59bb225",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vs_68b76a37d41c8191885888e3b7e5cb1f'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup Agent",
   "id": "4f2740a3f1441d15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:06:49.324949Z",
     "start_time": "2025-09-02T22:06:49.322583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from openai import OpenAI\n",
    "from agents import Agent, FileSearchTool, Runner, trace\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "Process_Knowledge_Base = Agent(\n",
    "        name=\"Process_Knowledge_Base\",\n",
    "        instructions=\"You are a helpful agent. You answer only based on the information in the vector store. Provide all citations with footnotes at the end of the answer.\",\n",
    "        model=os.getenv(\"LLM_MODEL\"),\n",
    "        tools=[\n",
    "            FileSearchTool(\n",
    "                max_num_results=20,\n",
    "                vector_store_ids=[vs.id],\n",
    "                include_search_results=True,\n",
    "            )\n",
    "        ],\n",
    "    )"
   ],
   "id": "a527bacf74c75125",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-02T22:09:13.904083Z",
     "start_time": "2025-09-02T22:06:49.344553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "message = \"Go through all the documents. Identify which individuals are involved in Identity Access & Management and provide a final headcount. And which of those roles can be augmented by Generative AI.\"\n",
    "result = await Runner.run(Process_Knowledge_Base, message)\n",
    "display(Markdown(result.final_output))"
   ],
   "id": "bf5707c5b80432ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[non-fatal] Tracing client error 401: {\n",
      "  \"error\": {\n",
      "    \"message\": \"Your authentication token is not from a valid issuer.\",\n",
      "    \"type\": \"invalid_request_error\",\n",
      "    \"param\": null,\n",
      "    \"code\": \"invalid_issuer\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Here’s what I found across the uploaded material.\n\nIndividuals involved in Identity Access & Management (IAM) — by role/record\nCore IAM organization (Identity & Access Management Division)\n- Identity and Access Management Technical Director III — User Access Management and Engagement (Teammate) — 1 individual. \n- Identity and Access Management Technical Director III — Client Identity Access Management (Teammate) — 1 individual. \n- Identity and Access Management Technical Director II — Authentication & Authorization Management (Teammate) — 1 individual. \n- Identity and Access Management Technical Director I — Identity & Access Management Manager I (Teammate) — 1 individual. \n- Identity and Access Management Technical Director I — Role Based Access Control Lead Strategist, Access/Role Management (Teammate) — 1 individual.  \n- Enterprise Tech – Cyber Security Analyst — User Access Lifecycle Support > Application Provisioning (Contingent workers; three distinct SOW records) — 3 individuals.   \n\nOther individuals performing IAM activities (outside the IAM org)\n- Worker Hash 4097E6E5… — Identity Access Management and “Centralize Access Administration.” \n- Worker Hash 19405D4D… — Identity Access Management and “Centralize Access Administration.” \n- Worker Hash 8061160C… — Identity Access Management and “Centralize Access Administration.” \n- Worker Hash C8DAD3DF… — Identity Access Management and “Centralize Access Administration.” \n\nNotes on potentially overlapping records\n- A workforce-allocation record shows another IAM Technical Director III focused on governance, controls, standards, metrics, and compliance within the IAM Division. Because this dataset is separate and de-identified, and may overlap with the entries above, I did not add it to the core headcount to avoid double counting. \n\nFinal headcount\n- Core IAM organization (Identity & Access Management Division): 8 individuals (5 teammates + 3 contingent).\n- Additional individuals performing IAM activities outside the IAM org: 4 individuals.\n- Total involved in IAM across the documents (core + outside): 12 individuals.\n\nWhich of these roles can be augmented by Generative AI\n- IAM Cloud Engineering: The team is explicitly driving “continued improvement for CICD and automation” and “leveraging AI to reduce engineering time.” This team’s scope includes Azure Active Directory (Entra), Role Management, Lifecycle items, CI/CD integration for IAM controls, and Privileged Access — all flagged as areas for optimization and automation. \n- AWS IAM Engineering & Operations: This team likewise highlights “continued improvement for CICD and automation” and the need for “additional upskilling for AI technology as related to IAM functions and automated workflow to limit human interaction,” indicating clear opportunities for GenAI augmentation across their IAM engineering and operations responsibilities. \n- Role Management/Access Modeling: The RBAC and role life-cycle work (e.g., “Role Design/Mining” and “Access Analysis and Reporting”) sits within IAM scope and is tied to the teams above that are already planning to leverage AI, making it a candidate for GenAI-assisted analysis/automation per the team charters.  \n\nFootnotes\n- Masked IAM teammate record — Identity & Access Management Technical Director I (User Access Lifecycle Support org details). \n- Masked IAM teammate record — Identity & Access Management Technical Director II (Authentication & Authorization Management). \n- Masked IAM teammate record — Identity & Access Management Technical Director III (User Access Management and Engagement). \n- Masked IAM teammate record — Identity & Access Management Technical Director III (Client Identity Access Management). \n- Masked IAM teammate record — Identity & Access Management Technical Director I (RBAC Lead Strategist; Access/Role Management). \n- Role Management activity definition (Access Model/Role Mining/SoD/Access Analysis). \n- Contingent worker records — IAM Provisioning/Deprovisioning & Access Mgmt (User Access Lifecycle Support > Application Provisioning).   \n- Worker-alignments performing IAM activities (outside IAM org).  \n- Workforce-allocation entry — IAM Technical Director III with IAM governance/metrics focus (possible overlap). \n- IAM Cloud Engineering team charter noting AI/automation for CICD and IAM controls. \n- AWS IAM Engineering & Operations team charter noting AI upskilling and automation to limit human interaction. "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
